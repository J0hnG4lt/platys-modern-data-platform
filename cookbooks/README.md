# Modern Data Platform Cookbooks

Inhere we are documenting cookbooks on how to use the platform:

 * **Airflow**
   * Schedule and Run Simple Scala Spark Application

 * **Presto**
   * [Presto and Delta Lake](./doc/delta-lake-and-presto/)
   * [Querying HDFS data using Presto](./doc/querying-hdfs-with-presto/)
   * Querying S3 data (MinIO) using Presto
   * Querying data in RDBMS
   * Joining data between RDBMS and MinIO

 * **MQTT**
   * Confluent MQTT Proxy 

 * **Spark**
   * [Run Java Spark Application using `spark-submit`](./doc/run-spark-simple-app-java-submit)
   * [Run Java Spark Application using Docker](./doc/run-spark-simple-app-java-docker)
   * [Run Scala Spark Application using `spark-submit`](./doc/run-spark-simple-app-scala-submit)
   * [Run Scala Spark Application using Docker](./doc/run-spark-simple-app-scala-docker)
   * [Run Python Spark Application using `spark-submit`](./doc/run-spark-simple-app-python-submit)
   * [Run Python Spark Application using Docker](./doc/run-spark-simple-app-python-docker)   
   * [Spark and Hive Metastore](./doc/spark-and-hive-metastore/)
   * [Spark with external S3](./doc/spark-with-external-s3)

 * **Hadoop HDFS**
   * [Querying HDFS data using Presto](./doc/querying-hdfs-with-presto/)
   * [Using HDFS data with Spark Data Frame](./doc/using-hdfs-with-spark/)
 
 * **Livy**
   * [Submit Spark Application over Livy](./doc/run-spark-simple-app-scala-livy)

 * **StreamSets Data Collector**
   * [Consume a binary file and send it as Kafka message](./doc/streamsets-binary-file-to-kafka) 
version: "2.1"

services:
  zookeeper-1:
    image: confluentinc/cp-zookeeper:5.2.2
    container_name: zookeeper-1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: always

  broker-1:
    image: confluentinc/cp-kafka:5.2.2
    container_name: broker-1
    depends_on:
      - zookeeper-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${PUBLIC_HOST_IP}:9092'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 9994
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9994'
      KAFKA_JMX_HOSTNAME: 'broker-1'      
    restart: always

  broker-2:
    image: confluentinc/cp-kafka:5.2.2
    container_name: broker-2
    depends_on:
      - zookeeper-1
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${DOCKER_HOST_IP}:9093'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 9993
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9993'
      KAFKA_JMX_HOSTNAME: 'broker-2'            
    restart: always

  broker-3:
    image: confluentinc/cp-enterprise-kafka:5.2.2
    container_name: broker-3
    depends_on:
      - zookeeper-1
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${PUBLIC_HOST_IP}:9094'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 9992
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9992'
      KAFKA_JMX_HOSTNAME: 'broker-3'
    restart: always
      
  schema-registry:
    image: confluentinc/cp-schema-registry:5.2.2
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - zookeeper-1
      - broker-1
    ports:
      - "18081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper-1:2181'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: 'GET,POST,PUT,OPTIONS'
    restart: always
      
  connect-1:
    image: confluentinc/cp-kafka-connect:5.2.2
    container_name: connect-1
    depends_on:
      - zookeeper-1
      - broker-1
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker-1:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect-1
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/custom-plugins"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - $PWD/kafka-connect:/etc/kafka-connect/custom-plugins
    restart: always

  connect-2:
    image: confluentinc/cp-kafka-connect:5.2.2
    container_name: connect-2
    depends_on:
      - zookeeper-1
      - broker-1
      - schema-registry
    ports:
      - "8084:8084"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker-1:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect-2
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/custom-plugins"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - $PWD/kafka-connect:/etc/kafka-connect/custom-plugins
    restart: always

  ksql-server-1:
    image: confluentinc/cp-ksql-server:5.2.2
    container_name: ksql-server-1
    ports:
      - '18088:8088'
    depends_on:
      - broker-1
      - schema-registry
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties"
      KSQL_BOOTSTRAP_SERVERS: "broker-1:9092,broker-2:9093"
      KSQL_HOST_NAME: ksql-server-1
      KSQL_APPLICATION_ID: "kafka-demo"
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      # Schema Registry using HTTPS
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_KSQL_SERVICE_ID: "kafka-demo"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
      KSQL_KSQL_SERVER_UI_ENABLED: "false"
    volumes:
      - $PWD/ksql:/etc/ksql/ext      
    restart: always

  ksql-server-2:
    image: confluentinc/cp-ksql-server:5.2.2
    container_name: ksql-server-2
    ports:
      - '18089:8088'
    depends_on:
      - broker-1
      - schema-registry
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties"
      KSQL_BOOTSTRAP_SERVERS: "broker-1:9092,broker-2:9093"
      KSQL_HOST_NAME: ksql-server-2
      KSQL_APPLICATION_ID: "kafka-demo"
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      # Schema Registry using HTTPS
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_KSQL_SERVICE_ID: "kafka-demo"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
      KSQL_KSQL_SERVER_UI_ENABLED: "false"
    volumes:
      - $PWD/ksql:/etc/ksql/ext      
    restart: always

  kafka-rest-1:
    image: confluentinc/cp-kafka-rest:5.2.2
    container_name: rest-proxy-1
    depends_on:
      - broker-1
      - schema-registry
    ports:
      - "8086:8086"
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: '${DOCKER_HOST_IP}:2181'
      KAFKA_REST_LISTENERS: 'http://0.0.0.0:8086'
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      KAFKA_REST_HOST_NAME: 'rest-proxy'
    restart: always
 
  kafka-mqtt-1:
    image: confluentinc/cp-kafka-mqtt:5.2.2
    hostname: mqtt-proxy
    ports:
      - "1882:1882"
    environment:
      KAFKA_MQTT_TOPIC_REGEX_LIST: 'truck_position:.*position,truck_engine:.*engine'
      KAFKA_MQTT_LISTENERS: 0.0.0.0:1882
      KAFKA_MQTT_BOOTSTRAP_SERVERS: PLAINTEXT://broker-1:9092,broker-2:9093
      KAFKA_MQTT_CONFLUENT_TOPIC_REPLICATIN_FACTOR: 1

  streamsets:
    image: trivadis/streamsets-kafka-hadoop-aws
    container_name: streamsets
    hostname: streamsets
    ports:
      - "18630:18630"
    restart: always

  nifi:
    image: apache/nifi
    container_name: nifi
    hostname: nifi
    ports:
      - "38080:8080"
    restart: always

  namenode:
    image: bde2020/hadoop-namenode:1.2.0-hadoop2.7.4-java8
    container_name: namenode
    volumes:
      - ./container-volume/namenode:/hadoop/dfs/name
    ports:
      - "50070:50070"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    healthcheck:
      interval: 5s
      retries: 100 
    restart: always

  datanode-1:
    image: bde2020/hadoop-datanode:1.2.0-hadoop2.7.4-java8
    container_name: datanode-1
    volumes:
      - ./container-volume/datanode:/hadoop/dfs/data
    ports:
      - "50075:50075"
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      interval: 5s
      retries: 100 
    restart: always

  spark-master:
    image: bde2020/spark-master:2.3.1-hadoop2.7 
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    env_file:
      - ./conf/hadoop-hive.env  
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
    restart: always

  spark-worker-1:
    image: bde2020/spark-worker:2.3.2-hadoop2.7
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    env_file:
      - ./conf/hadoop-hive.env      
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_LOCAL_IP=${PUBLIC_IP}
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
    restart: always
   
  spark-worker-2:
    image: bde2020/spark-worker:2.3.2-hadoop2.7
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    env_file:
      - ./conf/hadoop-hive.env      
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_LOCAL_IP=${PUBLIC_IP}
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
    restart: always

  spark-thrift-server:
    image: bde2020/spark-master:2.3.1-hadoop2.7 
    container_name: spark-thrift-server
    ports:
      - "10001:10000"
    env_file:
      - ./conf/hadoop-hive.env  
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
    command: bash -c "sleep 2m && /spark/sbin/start-thriftserver.sh && tail -f /spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-*.out"

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    ports:
      - "10000:10000"
    env_file:
      - ./conf/hadoop-hive.env
    environment:
      - "HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore"
      - "SERVICE_PRECONDITION=hive-metastore:9083"
    restart: always

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    ports:
      - "9083:9083"
    env_file:
      - ./conf/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      - "SERVICE_PRECONDITION=namenode:50070 datanode-1:50075 hive-metastore-postgresql:5432"
    restart: always
  
  hive-metastore-postgresql:
    container_name: hive-metastore-postgresql
    image: bde2020/hive-metastore-postgresql:2.3.0
    restart: always
         
  zeppelin:
    image: trivadis/apache-zeppelin:0.8.1-hadoop-2.7.0-spark-2.3.2
    container_name: zeppelin 
    ports:
      - 38081:8080
#    volumes:
#      - ./notebook:/opt/zeppelin/notebook
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SPARK_MASTER: "spark://spark-master:7077"
      MASTER: "spark://spark-master:7077"
#      SPARK_SUBMIT_OPTIONS: "--jars /opt/sansa-examples/jars/sansa-examples-spark.jar --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrator=net.sansa_stack.owl.spark.dataset.UnmodifiableCollectionKryoRegistrator"
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
    depends_on:
      namenode:
        condition: service_healthy
    restart: always

  hue:
    image: gethue/hue:4.4.0
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8888:8888"
#    command: ./build/env/bin/hue runserver_plus 0.0.0.0:8888
    volumes:
      - ./conf/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    restart: always

  solr:
    image: solr
    container_name: solr
    ports:
      - "8983:8983"
    restart: always

  schema-registry-ui:
    image: landoop/schema-registry-ui   
    container_name: schema-registry-ui
    depends_on:
      - broker-1
      - schema-registry
    ports:
      - "28002:8000"
    environment:
      SCHEMAREGISTRY_URL: 'http://${PUBLIC_IP}:8089'
    restart: always

  kafka-connect-ui:
    image: landoop/kafka-connect-ui:0.9.7
    container_name: kafka-connect-ui
    ports:
      - "28001:8000"
    environment:
      CONNECT_URL: "http://${PUBLIC_IP}:8083/,http://${PUBLIC_IP}:8084/"
      PROXY: "true"
    depends_on:
      - connect-1
    restart: always

  kafka-manager:
    image: trivadisbds/kafka-manager
    container_name: kafka-manager
    hostname: kafka-manager
    depends_on:
      - zookeeper-1
      - broker-1
      - broker-2
      - broker-3
    ports:
      - "29000:9000"
    environment:
      ZK_HOSTS: 'zookeeper-1:2181'
      APPLICATION_SECRET: 'letmein'
    restart: always
    
  kafdrop:
    image: thomsch98/kafdrop:latest
    container_name: kafdrop
    ports:
      - "29020:9020" 
    environment:
      ZK_HOSTS: zookeeper-1:2181
      LISTEN: 9020
    restart: always

  kadmin:
    image: hasnat/kadmin
    container_name: kadmin
    ports:
      - "28080:8080"
    environment:
      ZOOKEEPER_HOST: zookeeper-1:2181
      KAFKA_HOST: broker-1:9092
      #SECURITY_PROTOCOL: SSL
      #TRUST_STORE_LOCATION: ssl/client.truststore.jks
      #TRUST_STORE_PASSWORD: password
      #KEY_STORE_LOCATION: ssl/server.keystore.jks
      #KEY_STORE_PASSWORD: password
      #KEY_PASSWORD: password
    restart: always
    
  ksqlui:
    image: matsumana/tsujun
    hostname: ksqlui
    container_name: ksqlui
    ports: 
      - "28083:8080"
    environment:
      - KSQL_SERVER=http://ksql-server-1
            
  adminer:
    image: adminer
    container_name: adminer
    ports:
      - 38080:8080
    restart: always
      
  mysql:
    image: mysql:8
    container_name: mysql
    ports:
      - 3306:3306
    environment:
      - MYSQL_DATABASE=sample
      - MYSQL_USER=sample
      - MYSQL_PASSWORD=sample
      - MYSQL_ROOT_PASSWORD=manager
      - MYSQL_LOG_CONSOLE=true
    restart: always
      
  postgresql:
    image: mujz/pagila
    container_name: postgresql
    environment:
      - POSTGRES_PASSWORD=sample
      - POSTGRES_USER=sample
      - POSTGRES_DB=sample
    restart: always

  mongodb:
    image: mongo:4
    container_name: mongodb
    ports: 
      - 27017:27017
    environment:
      - MONGO_INITDB_DATABASE=sample   
      - MONGO_INITDB_USERNAME=admin
      - MONGO_INITDB_PASSWORD=admin 
    volumes:
      # seeding scripts
      - ./conf/mongo-entrypoint:/docker-entrypoint-initdb.d 
    restart: always

  mongo-express:
    image: mongo-express  
    container_name: mongo-express
    ports: 
      - 38082:8081
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongodb   
    restart: always

  admin-mongo:
    image: adicom/admin-mongo  
    container_name: admin-mongo
    ports: 
      - 1234:1234
    restart: always
         
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.7.0
    hostname: elasticsearch
    container_name: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      xpack.security.enabled: "false"
      XPACK_SECURITY_ENABLED: "false"
      xpack.monitoring.enabled: "false"
    restart: always

  kibana:
    image: docker.elastic.co/kibana/kibana:6.7.0
    hostname: kibana
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    environment:
      xpack.security.enabled: "false"
      XPACK_SECURITY_ENABLED: "false"
      xpack.monitoring.enabled: "false"
      discovery.type: "single-node"
      elasticsearch.url: http://elasticsearch:9200
      server.host: "0.0.0.0"
      SERVER_HOST: "0.0.0.0"
      server.name: "kibana"
      SERVER_NAME: "kibana"
      XPACK_GRAPH_ENABLED: "false"
      XPACK_MONITORING_ENABLED: "false"
      XPACK_REPORTING_ENABLED: "false"
      XPACK_SECURITY_ENABLED: "false"
    command: [ "/bin/bash", "-c", "/usr/share/kibana/bin/kibana-plugin remove x-pack; /usr/local/bin/kibana-docker" ]
    restart: always
       
  axon-server:
    image: axoniq/axonserver:4.1 
    container_name: axon-server  
    hostname: axon-server
    ports:
      - 8024:8024
      - 8124:8124
    environment:
      - AXONSERVER_HOSTNAME=axon-server
      - AXONSERVER_EVENTSTORE=/eventstore
      - AXONSERVER_CONTROLDB=/controldb
      - AXONSERVER_HTTP_PORT=8024
      - AXONSERVER_GRPC_PORT=8124
    restart: always
     
  activemq:
    image: rmohr/activemq
    container_name: activemq
    ports:
      # mqtt
      - "1883:1883"
      # amqp
      - "5672:5672"
      # ui
      - "8161:8161"
      # stomp
      - "61613:61613"
      # ws
      - "61614:61614"
      # jms
      - "61616:61616"
    volumes:
      - ./container-volume/activemq/data:/opt/activemq/data
    restart: always
    
  burrow:
    image: gschmutz/linkedin-burrow:1.2.0
    container_name: burrow
    volumes:
      - ./conf/burrow:/etc/burrow/
      - ./tmp:/var/tmp/burrow
    ports:
      - 23002:8000
    depends_on:
      - zookeeper-1
      - broker-1
    restart: always
    
  burrow-ui:
    image: generalmills/burrowui
    container_name: burrow-ui
    ports:
      - 23000:3000
    environment:
      - BURROW_HOME="http://${PUBLIC_IP}:28000/v3/kafka"
    depends_on:
      - zookeeper-1
      - broker-1
    restart: always

  burrow-dashboard:
    image: joway/burrow-dashboard
    container_name: burrow-dashboard
    ports:
      - 23001:80
    environment:
      - BURROW_BACKEND="http://${PUBLIC_IP}:28000"
    depends_on:
      - zookeeper-1
      - broker-1
    restart: always
    
  kafkahq:
    image: tchiotludo/kafkahq
    container_name: kafkahq
    ports:
      - 28082:8080
    environment:
      KAFKAHQ_CONFIGURATION: |
        kafkahq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "broker-1:9092"
              schema-registry: "http://schema-registry:8085"
    depends_on:
      - broker-1
    restart: always

  redis:
    container_name: redis
    hostname: redis
    image: redis
    ports:
      - 6379:6379
    restart: always

  redis-commander:
    container_name: redis-commander
    hostname: redis-commander
    image: rediscommander/redis-commander:latest
    environment:
      - REDIS_HOSTS=local:redis:6379
    ports:
      - "38083:8081"
    restart: always
    
  tile38:
    image: tile38/tile38
    container_name: tile38
    ports:
      - "9851:9851"
    restart: always        
  
  atlas:
    image: trivads/apache-atlas
    container_name: atlas
    ports:
      - 21000:21000
    restart: always

  minio:
    hostname: minio
    image: minio/minio
    container_name: minio
    ports:
      - '9000:9000'
#    volumes:
#      - './minio/data/:/data'
#      - './minio/config:/root/.minio'
    environment:
      MINIO_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      MINIO_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    command: server /data
    restart: always
    
  portainer:
    image: portainer/portainer
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
#      - data_portainer:/data
#    environment:
#      - VIRTUAL_HOST=monitor.bioatlas.se
#      - VIRTUAL_PORT=9000
    ports:
      - 19000:9000   
    restart: always
        
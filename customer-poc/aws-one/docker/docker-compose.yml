version: "3.1"

services:

  # Zookeeper & Kafka ===============================================

  zookeeper-1:
    image: confluentinc/cp-zookeeper:5.3.0
    container_name: zookeeper-1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: always

  broker-1:
    image: confluentinc/cp-kafka:5.3.0
    container_name: broker-1
    depends_on:
      - zookeeper-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${PUBLIC_IP}:9092'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9994
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9994'
      KAFKA_JMX_HOSTNAME: 'broker-1'      
    restart: always

  broker-2:
    image: confluentinc/cp-kafka:5.3.0
    container_name: broker-2
    depends_on:
      - zookeeper-1
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${PUBLIC_IP}:9093'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9993
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9993'
      KAFKA_JMX_HOSTNAME: 'broker-2'            
    restart: always

  broker-3:
    image: confluentinc/cp-kafka:5.3.0
    container_name: broker-3
    depends_on:
      - zookeeper-1
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: 'r1'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://${PUBLIC_IP}:9094'
#      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9992
      KAFKA_JMX_OPTS: '-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9992'
      KAFKA_JMX_HOSTNAME: 'broker-3'
    restart: always
      
  schema-registry:
    image: confluentinc/cp-schema-registry:5.3.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - zookeeper-1
      - broker-1
    ports:
      - "28030:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper-1:2181'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: 'GET,POST,PUT,OPTIONS'
    restart: always
      
  connect-1:
    image: confluentinc/cp-kafka-connect:5.3.0
    container_name: connect-1
    depends_on:
      - zookeeper-1
      - broker-1
      - schema-registry
    ports:
      - "28013:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker-1:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect-1
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/custom-plugins"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - $PWD/kafka-connect:/etc/kafka-connect/custom-plugins
    restart: always

  connect-2:
    image: confluentinc/cp-kafka-connect:5.3.0
    container_name: connect-2
    depends_on:
      - zookeeper-1
      - broker-1
      - schema-registry
    ports:
      - "28014:8084"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker-1:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect-2
      CONNECT_REST_PORT: 8084
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper-1:2181'
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/custom-plugins"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    volumes:
      - $PWD/kafka-connect:/etc/kafka-connect/custom-plugins
    restart: always

  ksql-server-1:
    image: confluentinc/cp-ksql-server:5.3.0
    container_name: ksql-server-1
    ports:
      - '28031:8088'
    depends_on:
      - broker-1
      - schema-registry
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties"
      KSQL_BOOTSTRAP_SERVERS: "broker-1:9092,broker-2:9093"
      KSQL_HOST_NAME: ksql-server-1
      KSQL_APPLICATION_ID: "kafka-demo"
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      # Schema Registry using HTTPS
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_KSQL_SERVICE_ID: "kafka-demo"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
      KSQL_KSQL_SERVER_UI_ENABLED: "false"
    volumes:
      - $PWD/ksql:/etc/ksql/ext      
    restart: always

  ksql-server-2:
    image: confluentinc/cp-ksql-server:5.3.0
    container_name: ksql-server-2
    ports:
      - '28032:8088'
    depends_on:
      - broker-1
      - schema-registry
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties"
      KSQL_BOOTSTRAP_SERVERS: "broker-1:9092,broker-2:9093"
      KSQL_HOST_NAME: ksql-server-2
      KSQL_APPLICATION_ID: "kafka-demo"
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      # Schema Registry using HTTPS
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      KSQL_KSQL_SERVICE_ID: "kafka-demo"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
      KSQL_KSQL_SERVER_UI_ENABLED: "false"
    volumes:
      - $PWD/ksql:/etc/ksql/ext      
    restart: always

  kafka-rest-1:
    image: confluentinc/cp-kafka-rest:5.3.0
    container_name: rest-proxy-1
    depends_on:
      - broker-1
      - schema-registry
    ports:
      - "28012:8086"
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: '${DOCKER_HOST_IP}:2181'
      KAFKA_REST_LISTENERS: 'http://0.0.0.0:8086'
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      KAFKA_REST_HOST_NAME: 'rest-proxy'
    restart: always

  schema-registry-ui:
    image: landoop/schema-registry-ui   
    container_name: schema-registry-ui
    depends_on:
      - broker-1
      - schema-registry
    ports:
      - "28039:8000"
    environment:
      SCHEMAREGISTRY_URL: 'http://${PUBLIC_IP}:28030'
    restart: always

  kafka-connect-ui:
    image: landoop/kafka-connect-ui:0.9.7
    container_name: kafka-connect-ui
    ports:
      - "28038:8000"
    environment:
      CONNECT_URL: "http://${PUBLIC_IP}:28013/,http://${PUBLIC_IP}:28014/"
      PROXY: "true"
    depends_on:
      - connect-1
      - connect-2
    restart: always

  kafka-manager:
    image: trivadis/kafka-manager
    container_name: kafka-manager
    hostname: kafka-manager
    depends_on:
      - zookeeper-1
      - broker-1
      - broker-2
      - broker-3
    ports:
      - "28044:9000"
    environment:
      ZK_HOSTS: 'zookeeper-1:2181'
      APPLICATION_SECRET: 'abc123!'
    restart: always
    
  kafdrop:
    image: thomsch98/kafdrop:latest
    container_name: kafdrop
    ports:
      - "28045:9020" 
    environment:
      ZK_HOSTS: zookeeper-1:2181
      LISTEN: 9020
    restart: always
    
  kafkahq:
    image: tchiotludo/kafkahq
    container_name: kafkahq
    ports:
      - 28042:8080
    environment:
      KAFKAHQ_CONFIGURATION: |
        kafkahq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "broker-1:9092,broker-2:9093"
              schema-registry:
                url: "http://schema-registry:8081"
              connect:
                url: "http://connect-1:8083"
    depends_on:
      - broker-1
    restart: always

  zoonavigator:
    image: elkozmon/zoonavigator-web:0.5.1
    container_name: zoonavigator
    hostname: zoonavigator
    ports:
      - "28047:8010"
    environment:
      WEB_HTTP_PORT: 8010
      API_HOST: "zoonavigator-api"
      API_PORT: 9010
    depends_on:
     - zoonavigator-api
    restart: always
    
  zoonavigator-api:
    image: elkozmon/zoonavigator-api:0.5.1
    container_name: zoonavigator-api
    hostname: zoonavigator-api
    ports:
      - "28048:9010"
    environment:
      API_HTTP_PORT: 9010
    restart: always

  # Hadoop ===============================================

  namenode:
    image: trivadis/apache-hadoop-namenode:2.0.0-hadoop3.1.1-java8
    container_name: namenode
    hostname: namenode
    volumes:
      - ./container-volume/namenode:/hadoop/dfs/name
      - ./data-transfer:/tmp/data-transfer
    ports:
      - "28084:9870"
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./conf/hadoop.env
    restart: always

  datanode-1:
    image: trivadis/apache-hadoop-datanode:2.0.0-hadoop3.1.1-java8
    container_name: datanode-1
    volumes:
      - ./container-volume/datanode-1:/hadoop/dfs/data
    ports:
      - "28085:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./conf/hadoop.env
    restart: always

  datanode-2:
    image: trivadis/apache-hadoop-datanode:2.0.0-hadoop3.1.1-java8
    container_name: datanode-2
    volumes:
      - ./container-volume/datanode-2:/hadoop/dfs/data
    ports:
      - "28086:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./conf/hadoop.env
    restart: always

  resourcemanager:
    image: trivadis/apache-hadoop-resourcemanager:2.0.0-hadoop3.1.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    environment:
      - YARN_CONF_yarn_resourcemanager_webapp_address=${PUBLIC_IP}:8088
      - YARN_CONF_yarn_nodemanager_webapp_address=${PUBLIC_IP}:8042
      - YARN_CONF_yarn_timeline___service_webapp_address=${PUBLIC_IP}:28020
      - YARN_CONF_yarn_log_server_url=${PUBLIC_IP}:28020
    restart: always
  
  nodemanager:
    image: trivadis/apache-hadoop-nodemanager:2.0.0-hadoop3.1.1-java8
    container_name: nodemanager
    hostname: nodemanager
    ports:
      - "8042:8042"
    depends_on:
      - namenode
      - datanode-1
      - datanode-2
    env_file:
      - ./conf/hadoop.env
    environment:
      - YARN_CONF_yarn_resourcemanager_webapp_address=${PUBLIC_IP}:8088
      - YARN_CONF_yarn_nodemanager_webapp_address=${PUBLIC_IP}:8042
      - YARN_CONF_yarn_timeline___service_webapp_address=${PUBLIC_IP}:28020
      - YARN_CONF_yarn_log_server_url=${PUBLIC_IP}:28020
    restart: always

  historyserver:
    image: trivadis/apache-hadoop-historyserver:2.0.0-hadoop3.1.1-java8
    container_name: historyserver
    hostname: historyserver
    ports:
      - "28020:8188"
    depends_on:
      - namenode
      - datanode-1
    env_file:
      - ./conf/hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode-1:9864 datanode-2:9864 resourcemanager:8088"      
    restart: always 

  hadoop-client:
    image: trivadis/apache-hadoop-client:2.0.0-hadoop3.1.1-java8
    container_name: hadoop-client
    hostname: hadoop-client
    env_file:
      - ./conf/hadoop.env
    command: tail -f /dev/null
    restart: always

  hadoop-setup:
    image: trivadis/apache-hadoop-client:2.0.0-hadoop3.1.1-java8
    hostname: hadoop-setup
    container_name: hadoop-setup
    env_file:
      - ./conf/hadoop.env
    depends_on:
      - namenode
    command:
      - bash 
      - -c 
      - |
        echo "Waiting for Hadoop Namenode to start listening on connect..."
        while [ $$(curl -s -o /dev/null -w %{http_code} http://namenode:9870) -ne 200 ] ; do 
          echo -e $$(date) " Hadoop Namenode state: " $$(curl -s -o /dev/null -w %{http_code} http://namenode:9870) " (waiting for 200)"
          sleep 5 
        done
        nc -vz namenode 9870
        echo -e "\n--\n+> Creating Hadoop Folders"
        hadoop fs -mkdir -p /spark/logs
        sleep infinity

  # Spark ===============================================

  spark-master:
    image: trivadis/apache-spark-master:2.4.3-hadoop3.1 
    container_name: spark-master
    hostname: spark-master
    ports:
      - 6066:6066
      - 7077:7077
      - 28076:8080
    env_file:
      - ./conf/hadoop.env  
    environment:
      - SPARK_PUBLIC_DNS=${PUBLIC_IP}
      - INIT_DAEMON_STEP=setup_spark
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-worker-1:
    image: trivadis/apache-spark-worker:2.4.3-hadoop3.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "28077:28077"
    env_file:
      - ./conf/hadoop.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: "28077"
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-worker-2:
    image: trivadis/apache-spark-worker:2.4.3-hadoop3.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "28078:28078"
    env_file:
      - ./conf/hadoop.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: "28078"
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-worker-3:
    image: trivadis/apache-spark-worker:2.4.3-hadoop3.1
    container_name: spark-worker-3
    hostname: spark-worker-3
    depends_on:
      - spark-master
    ports:
      - "28079:28079"
    env_file:
      - ./conf/hadoop.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: "28079"
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-history:
    image: trivadis/apache-spark-worker:2.4.3-hadoop3.1
    command: /spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    container_name: spark-history
    hostname: spark-history
    expose:
      - 18080
    ports:
      - 28072:18080
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-thrift-server:
    image: trivadis/apache-spark-master:2.4.3-hadoop3.1 
    container_name: spark-thrift-server
    ports:
      - "28073:10000"
    env_file:
      - ./conf/hadoop.env  
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    command: bash -c "sleep 2m && /spark/sbin/start-thriftserver.sh && tail -f /spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-*.out"
    restart: always 
    
  livy:
    image: trivadis/apache-livy
    container_name: livy
    hostname: livy
    env_file:
      - ./conf/hadoop.env 
    volumes:
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    ports:
      - "28021:8998"
    environment:
      - SPARK_MASTER=yarn
      - DEPLOY_MODE=cluster
    restart: always      
    
  # Hive ===============================================
    
  hive-server:
    image: johannestang/hive:2.3.4-postgresql-metastore-s3
    container_name: hive-server
    hostname: hive-server
    ports:
      - "28027:10000"
      - "28028:10002"
    env_file:
      - ./conf/hadoop.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
#      HDFS_CONF_fs_s3a_access_key: ${MINIO_ACCESS_KEY}
#      HDFS_CONF_fs_s3a_secret_key: ${MINIO_SECRET_KEY}
    restart: always

  hive-metastore:
    image: johannestang/hive:2.3.4-postgresql-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - "28026:9083"
    env_file:
      - ./conf/hadoop.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      - "SERVICE_PRECONDITION=namenode:9870 datanode-1:9864 hive-metastore-db:5432"
    restart: always
  
  hive-metastore-db:
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    image: bde2020/hive-metastore-postgresql:2.3.0
    restart: always
    
  # Governance ===============================================
    
  # Data Science Tools ===============================================

  hue:
    image: gethue/hue:4.4.0
    container_name: hue
    hostname: hue
    dns: 8.8.8.8
    ports:
      - "28043:8888"
    volumes:
      - ./conf/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    depends_on:
      - hue-db
      - solr
    restart: always

  hue-db:
    image: postgres:10
    container_name: hue-db
    hostname: hue-db
    environment:
      POSTGRES_DB: hue
      POSTGRES_PASSWORD: hue
      POSTGRES_USER: hue
    restart: always

  # .s3cfg is needed in order to use the s3cmd from a sh directive
  zeppelin:
    image: trivadis/apache-zeppelin:0.8.2-hadoop-3.1.1-spark-2.4.3
    container_name: zeppelin
    hostname: zeppelin
    ports:
      - "28055:8080"    
#      - "4040:4040"
#      - "42331:42331"    
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      # set spark-master for Zeppelin interpreter
      MASTER: "spark://spark-master:7077"
      SPARK_DRIVER_HOST: zeppelin
      SPARK_DRIVER_BINDADDRESS: "0.0.0.0"
      PYSPARK_PYTHON: "python3"
      SPARK_SUBMIT_OPTIONS: "--packages org.apache.commons:commons-lang3:3.5"
    volumes:
      - ./conf/hive-site.xml:/spark/conf/hive-site.xml
      - ./conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
      - ./conf/zeppelin/shiro.ini:/opt/zeppelin/conf/shiro.ini
      - ./conf/zeppelin/interpreter-setting.json:/opt/zeppelin/interpreter/spark/interpreter-setting.json
      - './conf/s3cfg:/root/.s3cfg'
    restart: always

  # make sure to get Spark to fit with the version of spark master 
  # - 2.4.0 tag 59b402ce701d
  # - 2.4.3 tag abdb27a6dfbb
  jupyter:
    image: jupyter/all-spark-notebook:abdb27a6dfbb
    container_name: jupyter
    hostname: jupyter
    ports: 
      - "28060:8888"
    environment:
      JUPYTER_ENABLE_LAB: "true"
      JUPYTER_TOKEN: "abc123!"
      GRANT_SUDO: "true"
      TINI_SUBREAPER: "true"
    restart: always
  
  # NoSQL ===============================================

  # Needed for Hue
  solr:
    image: solr
    container_name: solr
    ports:
      - "28081:8983"
    restart: always
         
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.7.0
    hostname: elasticsearch
    container_name: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      xpack.security.enabled: "false"
      XPACK_SECURITY_ENABLED: "false"
      xpack.monitoring.enabled: "false"
    restart: always

  kibana:
    image: docker.elastic.co/kibana/kibana:6.7.0
    hostname: kibana
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - "28006:5601"
    environment:
      xpack.security.enabled: "false"
      XPACK_SECURITY_ENABLED: "false"
      xpack.monitoring.enabled: "false"
      discovery.type: "single-node"
      elasticsearch.url: http://elasticsearch:9200
      server.host: "0.0.0.0"
      SERVER_HOST: "0.0.0.0"
      server.name: "kibana"
      SERVER_NAME: "kibana"
      XPACK_GRAPH_ENABLED: "false"
      XPACK_MONITORING_ENABLED: "false"
      XPACK_REPORTING_ENABLED: "false"
      XPACK_SECURITY_ENABLED: "false"
    command: [ "/bin/bash", "-c", "/usr/share/kibana/bin/kibana-plugin remove x-pack; /usr/local/bin/kibana-docker" ]
    restart: always
      

  # RDBMS ===============================================

  # Microsoft SQL Server Express
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2017-latest-ubuntu 
    hostname: sqlserver
    container_name: sqlserver
    ports:
      - "1433:1433"
    environment:
      ACCEPT_EULA: "Y"
      SA_PASSWORD: "SqlServerAdmin123!"
      MSSQL_PID: "Express"     
    restart: always
          
  #  Data Integration ===============================================

  streamsets:
    image: trivadis/streamsets-kafka-hadoop-aws
    container_name: streamsets
    hostname: streamsets
    ports:
      - "28029:18630"
    environment:
      SDC_OFFSET_DIRECTORY: /data/custom-offset-el
    volumes:
#      - /data/streamsets/data:/data:Z
      - ./streamsets-extras/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/postgresql-42.2.6.jar:/opt/streamsets-datacollector-3.11.0/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/postgresql-42.2.6.jar:Z
      - ./streamsets-extras/libs-common-lib:/opt/streamsets-datacollector-3.11.0/libs-common-lib:Z
      - ./streamsets-extras/user-libs:/opt/streamsets-datacollector-user-libs:Z
    restart: always
      
  #  Object Store (S3) ===============================================

  minio:
    hostname: minio
    image: minio/minio
    container_name: minio
    ports:
      - '28083:9000'
#    volumes:
#      - './minio/data/:/data'
#      - './minio/config:/root/.minio'
    environment:
      MINIO_ACCESS_KEY: ${OBJECT_STORAGE_ACCESS_KEY}
      MINIO_SECRET_KEY: ${OBJECT_STORAGE_SECRET_KEY}
    command: server /data
    restart: always
    
  awscli:
    image: xueshanf/awscli
    container_name: awscli
    hostname: awscli
    volumes:
      - './conf/s3cfg:/root/.s3cfg'
      - './data-transfer:/tmp/data-transfer'
#      - './minio/config:/root/.minio'
    environment:
      AWS_ACCESS_KEY_ID: ${OBJECT_STORAGE_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${OBJECT_STORAGE_SECRET_KEY}
    command: tail -f /dev/null
    restart: always
  
  object-storage-setup:
    image: xueshanf/awscli
    container_name: object-storage-setup
    hostname: object-storage-setup
    volumes:
      - './conf/s3cfg:/root/.s3cfg'
    environment:
      AWS_ACCESS_KEY_ID: ${OBJECT_STORAGE_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${OBJECT_STORAGE_SECRET_KEY}
    command:
      - bash 
      - -c 
      - |
        echo -e "\n--\n+> Creating Object Storage Folder"
        s3cmd mb s3://spark-logs
        sleep infinity
        
#  FTP ===============================================
        
  ftp:
    image: stilliard/pure-ftpd
    container_name: ftp
    hostname: ftp
    environment:
#      - PUBLICHOST="192.168.73.86"
      - PUBLICHOST=${DOCKER_HOST_IP}
      - FTP_USER_NAME=orderproc
      - FTP_USER_PASS=admin
      - FTP_USER_HOME=/home/ftp-data
      - FTP_MAX_CLIENTS=9
    ports:
      - "21:21"
      - "30000-30009:30000-30009"
    secrets:
      - admin_password
    restart: always

  filezilla:
    image: jlesage/filezilla
    container_name: filezilla
    volumes: 
      - ./data-transfer:/data-transfer
    ports:
      - "28008:5800"
      - "28009:5900"
#    volumes:
#      - "/docker/appdata/filezilla:/config:rw"
#      - "/files:/storage:rw"
    environment:
      - VNC_PASSWORD=admin123 

#  Container UI ===============================================

  portainer:
    image: portainer/portainer
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
#      - data_portainer:/data
    ports:
      - 28071:9000   
    restart: always  

secrets:
   admin_password:
     file: secrets/admin_password.txt
